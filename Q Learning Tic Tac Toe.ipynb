{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning in Tic Tac Toe\n",
    "\n",
    "This code file is pretty much a code-with-answer-key session, where I try to recreate something others have done while having their own code to heavily reference. The point of this is to explore Q Learning. The code I structured my game after is [here](https://www.kaggle.com/slobo777/tic-tac-toe-agent-using-q-learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's make a tic tac toe game first. A move is defined to be simply an override of the original board. The object does not provide good interface for human player, but is suited for the `Agent` to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    empty = ' '\n",
    "    first_move = 'X'\n",
    "    second_move = 'O'\n",
    "\n",
    "    def __init__(self, board_size=3):\n",
    "        # board represented as a size*size length string, each char\n",
    "        # being one of ' ', 'X', or 'O'\n",
    "        self.board = \" \" * (board_size**2) \n",
    "        self.player = Game.first_move\n",
    "        self.winner = None \n",
    "    \n",
    "    @property\n",
    "    def board_len(self):\n",
    "        \"\"\" Length of one side of the board\n",
    "        \"\"\"\n",
    "        return int(len(self.board)**0.5)\n",
    "\n",
    "    def possible_boards(self):\n",
    "        \"\"\" Gives a list of all possible configurations of the board\n",
    "            after the current player moves\n",
    "        \"\"\"\n",
    "        all_boards = []\n",
    "        for i in range(len(self.board)):\n",
    "            if self.board[i] == Game.empty:\n",
    "                all_boards.append(self.board[:i] + self.player + self.board[i+1:])\n",
    "        return all_boards\n",
    "    \n",
    "    def move(self, board):\n",
    "        \"\"\" Given a new state of the board, makes a move. Enforces \n",
    "            tic tac toe rules\n",
    "        \"\"\"\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"The game is already completed. Cannot make another move\")\n",
    "        if board not in self.possible_boards():\n",
    "            raise Exception(\"Cannot make move {} to {} for player {}\".format(self.board, board, self.player))\n",
    "        \n",
    "        self.board = board\n",
    "        self.winner = self.get_winner(board)\n",
    "        if self.winner:\n",
    "            self.player = None\n",
    "        elif self.player == Game.first_move:\n",
    "            self.player = Game.second_move\n",
    "        elif self.player == Game.second_move:\n",
    "            self.player = Game.first_move\n",
    "\n",
    "    def get_winner(self, board):\n",
    "        \"\"\" Takes a board and decides if there is a winner and which \n",
    "            player it is\n",
    "        \"\"\"\n",
    "        lines_to_be_checked = []\n",
    "        for i in range(self.board_len):\n",
    "            lines_to_be_checked.append(list(range(i*self.board_len, (i+1)*self.board_len)))\n",
    "            lines_to_be_checked.append(list(range(i, len(board), self.board_len)))\n",
    "        lines_to_be_checked.append(list(range(0,len(board),self.board_len+1)))\n",
    "        lines_to_be_checked.append(list(range(self.board_len-1, len(board)-1, self.board_len-1)))\n",
    "        \n",
    "        winner = None\n",
    "        \n",
    "        for line in lines_to_be_checked:\n",
    "            if all([board[i]==Game.first_move for i in line]):\n",
    "                winner = Game.first_move\n",
    "            elif all([board[i]==Game.second_move for i in line]):\n",
    "                winner = Game.second_move\n",
    "            \n",
    "        return winner\n",
    "    \n",
    "    def playable(self):\n",
    "        \"\"\" If the game is still able to be continued \n",
    "        \"\"\"\n",
    "        return (self.winner is None) and any(self.possible_boards())\n",
    "    \n",
    "    def empty_squares(self):\n",
    "        \"\"\" Returns index of all the empty squares\n",
    "        \"\"\"\n",
    "        return [i for i in range(len(self.board)) if self.board[i] == Game.empty]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"\"\"\n",
    "{} | {} | {}\n",
    "---------\n",
    "{} | {} | {}\n",
    "---------\n",
    "{} | {} | {}\n",
    "\"\"\"\n",
    "        return s.format(*self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's make an agent that could be trained! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, game_class, er=0.1, lr=0.5, player=\"X\"):\n",
    "        self.new_game = game_class # call self.new_game() to make a new game\n",
    "        self.er = er # the rate at which to try out new, potentially bad options\n",
    "        self.lr = lr # the rate at which the program learns from results\n",
    "        self.Q = defaultdict(lambda: 0.0) # the Q \"matrix\", key is a board cofig\n",
    "        self.player = player # the player the agent is playing for\n",
    "    \n",
    "    def select_move(self, game, learning=True):\n",
    "        \"\"\" Select the best move for the current player of the game, \n",
    "            regardless of who the Agent is playing for. If learning, \n",
    "            then experiment with random moves once in awhile\n",
    "        \"\"\"\n",
    "        def min_board(possible_boards):\n",
    "            m = min(possible_boards, key=lambda s: self.Q[s])\n",
    "            possibles = [p for p in possible_boards if self.Q[p] == self.Q[m]]\n",
    "            return random.choice(possibles)\n",
    "        \n",
    "        def max_board(possible_boards):\n",
    "            m = max(possible_boards, key=lambda s: self.Q[s])\n",
    "            possibles = [p for p in possible_boards if self.Q[p] == self.Q[m]]\n",
    "            return random.choice(possibles)\n",
    "        \n",
    "        possible_boards = game.possible_boards()\n",
    "        if game.player == self.player:\n",
    "            choose_board = max_board(possible_boards)\n",
    "        else: # choose the worst possible play for the player, which is the best for the opponent\n",
    "            choose_board = min_board(possible_boards)\n",
    "        \n",
    "        if learning:\n",
    "            if random.random() < self.er: # if experiment\n",
    "                choose_board = random.choice(possible_boards)\n",
    "\n",
    "        return choose_board\n",
    "    \n",
    "    def learn_from_move(self, game, move):\n",
    "        \"\"\" Update the Q matrix based on the result of the move\n",
    "        \"\"\"\n",
    "        game.move(move)\n",
    "        r = self.reward(game)\n",
    "        \n",
    "        next_state_value = 0.0\n",
    "        if game.playable():\n",
    "            next_state_value = self.Q[self.select_move(game, False)]\n",
    "        self.Q[move] = (1-self.lr)*self.Q[move] + self.lr * (r+next_state_value)\n",
    "    \n",
    "    def learn(self, epochs=1000):\n",
    "        \"\"\" Trains the agent \n",
    "        \"\"\"\n",
    "        for _ in range(epochs):\n",
    "            g = self.new_game()\n",
    "            while g.playable():\n",
    "                move = self.select_move(g)\n",
    "                self.learn_from_move(g, move)\n",
    "    \n",
    "    def reward(self, game):\n",
    "        \"\"\" Rewards the action if it leads to a win \n",
    "        \"\"\"\n",
    "        if game.winner is None:\n",
    "            return 0.0\n",
    "        if game.winner == self.player:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "    \n",
    "    def play(self, show_text=True):\n",
    "        \"\"\" Play a game where the AI does both sides \n",
    "        \"\"\"\n",
    "        g = self.new_game()\n",
    "        turn = 0\n",
    "        \n",
    "        if show_text:\n",
    "            print(\"Turn {}\".format(turn))\n",
    "            print(g)\n",
    "            \n",
    "        while g.playable():\n",
    "            move = self.select_move(g, False)\n",
    "            g.move(move)\n",
    "            turn += 1\n",
    "            \n",
    "            if show_text:\n",
    "                print(\"Turn {}\".format(turn))\n",
    "                print(g)\n",
    "            \n",
    "        if g.winner:\n",
    "            if show_text:\n",
    "                print(\"Winner is {}\".format(g.winner))\n",
    "            return g.winner\n",
    "        else:\n",
    "            if show_text:\n",
    "                print(\"Draw!\")\n",
    "            return '-'\n",
    "    \n",
    "    def stats(self, total_games=10000):\n",
    "        \"\"\" Empirically decide how well/biased the AI plays\n",
    "        \"\"\"\n",
    "        results = [self.play(False) for i in range(total_games)]\n",
    "        return {k: results.count(k)/(total_games/100) for k in ['X', 'O', '-']}\n",
    "    \n",
    "    def play_human(self):\n",
    "        \"\"\" Play an interactive game with a human player \n",
    "        \"\"\"\n",
    "        g = self.new_game()\n",
    "        turn = 0\n",
    "        while g.playable():\n",
    "            if g.player == self.player:\n",
    "                move = self.select_move(g, False)\n",
    "            else:\n",
    "                move = self.get_human_move(g)\n",
    "            \n",
    "            g.move(move)\n",
    "            turn += 1\n",
    "            \n",
    "            print(\"Turn {}\".format(turn))\n",
    "            print(g)\n",
    "            \n",
    "    def get_human_move(self, game):\n",
    "        \"\"\" IO function to get a move from the human \n",
    "        \"\"\"\n",
    "        allowed_moves = game.empty_squares()\n",
    "        print(allowed_moves)\n",
    "        human_move = None\n",
    "        while human_move is None:\n",
    "            index = int(input(\"Choose where to put your {}\".format(game.player)))\n",
    "            if index in allowed_moves:\n",
    "                human_move = game.board[:index] + game.player + game.board[index+1:]\n",
    "        return human_move\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code to train the agent and evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs\n",
      "{'-': 11.0, 'X': 59.0, 'O': 30.0}\n",
      "After 1000 epochs\n",
      "{'-': 10.2, 'X': 58.9, 'O': 30.9}\n",
      "After 2000 epochs\n",
      "{'-': 16.1, 'X': 51.4, 'O': 32.5}\n",
      "After 4000 epochs\n",
      "{'-': 51.9, 'X': 30.3, 'O': 17.8}\n",
      "After 8000 epochs\n",
      "{'-': 93.7, 'X': 5.7, 'O': 0.6}\n",
      "After 16000 epochs\n",
      "{'-': 99.8, 'X': 0.2, 'O': 0.0}\n",
      "After 32000 epochs\n",
      "{'-': 99.9, 'X': 0.1, 'O': 0.0}\n"
     ]
    }
   ],
   "source": [
    "a = Agent(Game, er=0.1, lr=1.0, player=\"X\")\n",
    "\n",
    "milestones = [0,1000,2000,4000,8000,16000,32000]\n",
    "for i, m in enumerate(milestones):\n",
    "    to_be_trained = m-milestones[i-1] if i!=0 else m\n",
    "    a.learn(to_be_trained)\n",
    "    print(\"After {} epochs\".format(m))\n",
    "    print(a.stats(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fight me bro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.play_human()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
